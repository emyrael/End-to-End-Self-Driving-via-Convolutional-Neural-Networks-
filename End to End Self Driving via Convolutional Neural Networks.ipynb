{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: End-to-End Self-Driving via Convolutional Neural Net- works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from skimage import io\n",
    "\n",
    "#uploading data\n",
    "data_name = 'driving_dataset'\n",
    "data_dir = os.listdir(data_name)\n",
    "#data description is in txt file so read it\n",
    "txt_file = [i for i in data_dir if i.find('.txt') >= 0]\n",
    "txt_file = txt_file[0]\n",
    "\n",
    "with open(os.path.join(data_name, txt_file), 'r') as f1:\n",
    "    markimg = f1.readlines()\n",
    "mark = [i[:-2] for i in mark]\n",
    "#In each line there are two words separated by blank. First word is img name and second is target variable value\n",
    "dataset = [i.split(' ') for i in mark]\n",
    "\n",
    "data_train_val = dataset[:-10000]\n",
    "data_train = data_train_val[:int(len(data_train_val)*0.8)]\n",
    "data_val = data_train_val[int(len(data_train_val)*0.8):]\n",
    "data_test = dataset[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning on full dataset is very very very very very long (especially random search later), so it would be better to reduce data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train[:min(len(data_train), 3000)]\n",
    "data_val = data_val[:min(len(data_val), 1000)]\n",
    "data_test = data_test[:min(len(data_test), 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If cuda is not available, training on full data might be vary long process\n",
    "#So if cuda is not available, we reduce size of datasets to reduce time of learning\n",
    "if not(torch.cuda.is_available()):\n",
    "    print('Cuda is not available so it would be better to reduce size of datasets')\n",
    "    data_train = data_train[:min(len(data_train), 1000)]\n",
    "    data_val = data_val[:min(len(data_val), 1000)]\n",
    "    data_test = data_test[:min(len(data_test), 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tImplement the Convolutional Neural Network Architecture proposed in the paper titled, \"End to End Learning for Self-Driving Cars\". The paper can be accessed here: https://arxiv.org/abs/1604.07316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#creating neural network class\n",
    "#Data in dataset and data in the paper has different img sizes so last two convolutions were made with stride 2 to reduce size\n",
    "class NetRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetRegressor, self).__init__()\n",
    "        #self.input_norm = nn.LayerNorm()\n",
    "        #convolutional layers\n",
    "        self.layer1 = nn.Conv2d(in_channels = 3, out_channels = 24, kernel_size = 5, stride = 2)\n",
    "        self.layer2 = nn.Conv2d(in_channels = 24, out_channels = 36, kernel_size = 5, stride = 2) \n",
    "        self.layer3 = nn.Conv2d(in_channels = 36, out_channels = 48, kernel_size = 5, stride = 2)\n",
    "        self.layer4 = nn.Conv2d(in_channels = 48, out_channels = 64, kernel_size = 3, stride = 2)\n",
    "        self.layer5 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 2)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        #linear layers\n",
    "        self.lin1 = nn.Linear(4608, 100)\n",
    "        self.lin2 = nn.Linear(100, 50)\n",
    "        self.lin3 = nn.Linear(50, 10)\n",
    "        self.lin4 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.input_norm(x)\n",
    "        x = (x-x.mean())/x.std() #Do normalization\n",
    "        #apply convolutional layers\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "        #x5 = self.flat(x5)\n",
    "        #x5 = torch.flatten(x5, start_dim = 2)\n",
    "        x5 = x5.view(x5.size(0), -1) #flatten\n",
    "        #Then apply linear layers\n",
    "        x6 = self.lin1(x5) \n",
    "        x7 = self.lin2(x6)\n",
    "        x8 = self.lin3(x7)\n",
    "        out = self.lin4(x8)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "Number of batches is 100\n",
      "Loss_train: 4149020.5038834633\n",
      "Loss_test: 3118.5548056650164\n",
      "Epoch number 1\n",
      "Number of batches is 100\n",
      "Loss_train: 4190.006430358887\n",
      "Loss_test: 2178.7175324058535\n",
      "Epoch number 2\n",
      "Number of batches is 100\n",
      "Loss_train: 3724.19593943278\n",
      "Loss_test: 1432.921285095215\n",
      "Epoch number 3\n",
      "Number of batches is 100\n",
      "Loss_train: 3463.870445696513\n",
      "Loss_test: 3035.1163006591796\n",
      "Epoch number 4\n",
      "Number of batches is 100\n",
      "Loss_train: 3225.475763804118\n",
      "Loss_test: 3866.173816833496\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "batch_size = 10\n",
    "n_epoch = 5\n",
    "train_err_list = []\n",
    "test_err_list = []\n",
    "#Make dataloaders that will split data to batches for learning\n",
    "trainLoader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "valLoader = torch.utils.data.DataLoader(data_val, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=1)\n",
    "#Check if cuda is available. If it is, then we will do all learning on gpu to make it much faster\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RegNet = NetRegressor()\n",
    "RegNet.to(device)\n",
    "lossMSE = nn.MSELoss(reduce = True, reduction = 'mean')\n",
    "optimizer = torch.optim.Adam(RegNet.parameters(), lr=0.01, weight_decay = 0.1)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch number {}'.format(epoch))\n",
    "    print('Number of batches is {}'.format(int(len(data_val)/batch_size)))\n",
    "    train_losses = []\n",
    "    RegNet.train() #Change neural network's mode to training mode\n",
    "    for i, d in enumerate(trainLoader):\n",
    "        #load data and make it right shape (batch size, number of channels, shape0, shape1)\n",
    "        in_data = np.array([io.imread(os.path.join(data_name, name)) for name in d[0]])\n",
    "        in_data = np.swapaxes(in_data, 1, 3)\n",
    "        #if gpu is available, learning will be on gpu\n",
    "        if device.type == 'cuda':\n",
    "            in_data = torch.cuda.FloatTensor(in_data)\n",
    "            target_data = np.array([float(i) for i in d[1]])\n",
    "            target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "        else:\n",
    "            in_data = torch.cuda.FloatTensor(in_data)\n",
    "            target_data = np.array([float(i) for i in d[1]])\n",
    "            target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = RegNet.forward(in_data)\n",
    "        loss = lossMSE(outputs, target_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.cpu().item())\n",
    "    train_losses = np.mean(train_losses)\n",
    "    train_err_list.append(train_losses)\n",
    "    print('Loss_train: {}'.format(train_losses))\n",
    "    \n",
    "    test_losses = []\n",
    "    RegNet.eval()\n",
    "    for i, d in enumerate(valLoader):\n",
    "        in_data = np.array([io.imread(os.path.join(data_name, name)) for name in d[0]])\n",
    "        in_data = np.swapaxes(in_data, 1, 3)\n",
    "        if device.type == 'cuda':\n",
    "            in_data = torch.cuda.FloatTensor(in_data)\n",
    "            target_data = np.array([float(i) for i in d[1]])\n",
    "            target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "        else:\n",
    "            in_data = torch.cuda.FloatTensor(in_data)\n",
    "            target_data = np.array([float(i) for i in d[1]])\n",
    "            target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "        outputs = RegNet.forward(in_data)\n",
    "        loss = lossMSE(outputs, target_data)\n",
    "        test_losses.append(loss.cpu().item())\n",
    "    \n",
    "    test_losses = np.mean(test_losses)\n",
    "    test_err_list.append(test_losses)\n",
    "    print('Loss_test: {}'.format(test_losses))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tReport one test RMSE for the test set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE loss is 39.39970518967743\n"
     ]
    }
   ],
   "source": [
    "final_losses = []\n",
    "for img, target in data_test:\n",
    "    in_data = np.array([io.imread(os.path.join(data_name, img))])\n",
    "    in_data = np.swapaxes(in_data, 1, 3)\n",
    "    if device.type == 'cuda':\n",
    "        in_data = torch.cuda.FloatTensor(in_data)\n",
    "        target_data = torch.cuda.FloatTensor(np.array([[float(target)]]))\n",
    "    else:\n",
    "        in_data = torch.Tensor(in_data)\n",
    "        target_data = torch.FloatTensor(np.array([[float(target)]]))\n",
    "    outputs = RegNet.forward(in_data)\n",
    "    loss = lossMSE(outputs, target_data)\n",
    "    final_losses.append(np.sqrt(loss.cpu().item()))\n",
    "\n",
    "print('Final RMSE loss is {}'.format(np.mean(final_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning, Regularization with Image Transformations \n",
    "The aim of this exercise is to further develop modifications on top, that can hopefully lead to performance gains over the architecture from the previous exercise.\n",
    "1.\tTune the associated hyperparameters such as batch_size, number_of_layers, kernel_sizes, learn- ing_rate, l1_regularization, l2_regularization coefficients etc. Either implement Random Search or Hyperband."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#creating neural network class\n",
    "#Data in dataset and data in the paper has different img sizes so last two convolutions were made with stride 2 to reduce size\n",
    "class VariousNet(nn.Module):\n",
    "    def __init__(self, n_layers = 5, kernel_sizes = 5, input_ch = 3, img_size = (256, 455)):\n",
    "        super(VariousNet, self).__init__()\n",
    "        \n",
    "        if n_layers <= 0:\n",
    "            raise \"Number of layers must be positive integer!\"\n",
    "        \n",
    "        n_lin_neurons1 = img_size[0]\n",
    "        n_lin_neurons2 = img_size[1]\n",
    "        n_lin_neurons3 = input_ch\n",
    "        \n",
    "        self.hidden = nn.ModuleList([])\n",
    "        \n",
    "        \n",
    "        if isinstance(kernel_sizes, int):\n",
    "            for i in range(n_layers):\n",
    "                if (int((n_lin_neurons1 - kernel_sizes)/2 + 1) <= 0) or (int((n_lin_neurons2 - kernel_sizes)/2 + 1) <= 0):\n",
    "                    print(\"You have chosen too much layers. Set number of layers to maximum equal \" + str(i))\n",
    "                    break\n",
    "                \n",
    "                if i == 0:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = input_ch, out_channels = 24,\n",
    "                                             kernel_size = int(kernel_sizes), stride = 2)\n",
    "                    n_lin_neurons3 = 24\n",
    "                elif i == 1:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 24, out_channels = 36,\n",
    "                                             kernel_size = int(kernel_sizes), stride = 2)\n",
    "                    n_lin_neurons3 = 36\n",
    "                elif i == 2:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 36, out_channels = 48,\n",
    "                                             kernel_size = int(kernel_sizes), stride = 2)\n",
    "                    n_lin_neurons3 = 48\n",
    "                elif i == 3:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 48, out_channels = 64,\n",
    "                                             kernel_size = int(kernel_sizes), stride = 2)\n",
    "                    n_lin_neurons3 = 64\n",
    "                else:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 64, out_channels = 64,\n",
    "                                             kernel_size = int(kernel_sizes), stride = 2)\n",
    "                    n_lin_neurons3 = 64\n",
    "                \n",
    "                n_lin_neurons1 = int((n_lin_neurons1 - kernel_sizes)/2 + 1)\n",
    "                n_lin_neurons2 = int((n_lin_neurons2 - kernel_sizes)/2 + 1)\n",
    "                \n",
    "                self.hidden.append(layer_to_add)\n",
    "                \n",
    "        elif isinstance(kernel_sizes, list):\n",
    "            self.hidden = nn.ModuleList()\n",
    "            for i in range(n_layers):\n",
    "                \n",
    "                if (int((n_lin_neurons1 - kernel_sizes[i])/2 + 1) <= 0) or (int((n_lin_neurons2 - kernel_sizes[i])/2 + 1) <= 0):\n",
    "                    print(\"You have chosen too much layers. Set number of layers to maximum equal \" + str(i))\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if i == 0:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = input_ch, out_channels = 24,\n",
    "                                             kernel_size = int(kernel_sizes[i]), stride = 2)\n",
    "                    n_lin_neurons3 = 24\n",
    "                elif i == 1:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 24, out_channels = 36,\n",
    "                                             kernel_size = int(kernel_sizes[i]), stride = 2)\n",
    "                    n_lin_neurons3 = 36\n",
    "                elif i == 2:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 36, out_channels = 48,\n",
    "                                             kernel_size = int(kernel_sizes[i]), stride = 2)\n",
    "                    n_lin_neurons3 = 48\n",
    "                elif i == 3:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 48, out_channels = 64,\n",
    "                                             kernel_size = int(kernel_sizes[i]), stride = 2)\n",
    "                    n_lin_neurons3 = 64\n",
    "                else:\n",
    "                    layer_to_add = nn.Conv2d(in_channels = 64, out_channels = 64,\n",
    "                                             kernel_size = int(kernel_sizes[i]), stride = 2)\n",
    "                    n_lin_neurons3 = 64\n",
    "                \n",
    "                n_lin_neurons1 = int((n_lin_neurons1 - kernel_sizes[i])/2 + 1)\n",
    "                n_lin_neurons2 = int((n_lin_neurons2 - kernel_sizes[i])/2 + 1)\n",
    "                    \n",
    "                \n",
    "                self.hidden.append(layer_to_add)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        #linear layers\n",
    "        self.lin1 = nn.Linear(n_lin_neurons1*n_lin_neurons2*n_lin_neurons3, 100)\n",
    "        self.lin2 = nn.Linear(100, 50)\n",
    "        self.lin3 = nn.Linear(50, 10)\n",
    "        self.lin4 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.input_norm(x)\n",
    "        x = (x-x.mean())/x.std() #Do normalization\n",
    "        #apply convolutional layers\n",
    "        out_conv = x\n",
    "        for i, conv1 in enumerate(self.hidden):\n",
    "            out_conv = self.hidden[i](out_conv)\n",
    "        out_conv = out_conv.view(out_conv.size(0), -1) #flatten\n",
    "        #Then apply linear layers\n",
    "        x1 = self.lin1(out_conv) \n",
    "        x2 = self.lin2(x1)\n",
    "        x3 = self.lin3(x2)\n",
    "        out = self.lin4(x3)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_process(params, data_train, data_val, n_epoch = 5):\n",
    "    #Make dataloaders that will split data to batches for learning\n",
    "    trainLoader = torch.utils.data.DataLoader(data_train, batch_size= int(params.get('batch_size', 5)),\n",
    "                                              shuffle=True, num_workers=1)\n",
    "    valLoader = torch.utils.data.DataLoader(data_val, batch_size=int(params.get('batch_size', 5)),\n",
    "                                             shuffle=False, num_workers=1)\n",
    "    #Check if cuda is available. If it is, then we will do all learning on gpu to make it much faster\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    n_layers = int(params.get('n_layers', 5))\n",
    "    kernel_sizes = params.get('kernel_sizes', 3)\n",
    "    if not isinstance(kernel_sizes, list):\n",
    "        kernel_sizes = int(kernel_sizes)\n",
    "    \n",
    "    RegNet = VariousNet(n_layers = n_layers, kernel_sizes = kernel_sizes)\n",
    "    RegNet.to(device)\n",
    "   \n",
    "    lossMSE = nn.MSELoss(reduce = True, reduction = 'mean')\n",
    "    optimizer = torch.optim.Adam(RegNet.parameters(), lr = params.get('learning_rate', 0.01),\n",
    "                                 weight_decay = params.get('l2_regularization', 0.1))\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch number {}'.format(epoch))\n",
    "        #print('Number of batches is {}'.format(int(len(data_val)/batch_size)))\n",
    "        train_losses = []\n",
    "        RegNet.train() #Change neural network's mode to training mode\n",
    "        for i, d in enumerate(trainLoader):\n",
    "            #load data and make it right shape (batch size, number of channels, shape0, shape1)\n",
    "            in_data = np.array([io.imread(os.path.join(data_name, name)) for name in d[0]])\n",
    "            in_data = np.swapaxes(in_data, 1, 3)\n",
    "            #if gpu is available, learning will be on gpu\n",
    "            if device.type == 'cuda':\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            else:\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = RegNet.forward(in_data)\n",
    "            loss = lossMSE(outputs, target_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.cpu().item())\n",
    "        train_losses = np.mean(train_losses)\n",
    "        train_err_list.append(train_losses)\n",
    "        print('Loss_train: {}'.format(train_losses))\n",
    "        \n",
    "        test_losses = []\n",
    "        RegNet.eval()\n",
    "        for i, d in enumerate(valLoader):\n",
    "            in_data = np.array([io.imread(os.path.join(data_name, name)) for name in d[0]])\n",
    "            in_data = np.swapaxes(in_data, 1, 3)\n",
    "            if device.type == 'cuda':\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            else:\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            outputs = RegNet.forward(in_data)\n",
    "            loss = lossMSE(outputs, target_data)\n",
    "            test_losses.append(loss.cpu().item())\n",
    "    \n",
    "        test_losses = np.mean(test_losses)\n",
    "        test_err_list.append(test_losses)\n",
    "        print('Loss_test: {}'.format(test_losses))\n",
    "\n",
    "    print('Finished Training')\n",
    "   \n",
    "    return RegNet, test_err_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomizedSearchArchitecture(grid_of_params, data_train, data_test, n_iter = 3):\n",
    "    grid_seaved = {key: np.random.choice(grid_of_params[key], n_iter, replace = True) for key in grid_of_params.keys()}\n",
    "    search_combinations = [{key: grid_seaved[key][i] for key in grid_seaved.keys()} for i in range(n_iter)]\n",
    "    best_model = VariousNet()\n",
    "    best_result = np.inf\n",
    "    \n",
    "    for i, comb in enumerate(search_combinations):\n",
    "        print(\"Combination number {}\".format(i))\n",
    "        net, err = learning_process(comb, data_train, data_test)\n",
    "        if err < best_result:\n",
    "            best_result = err\n",
    "            best_model = net\n",
    "    print('Search is finished!')\n",
    "    return best_model, best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination number 0\n",
      "Epoch number 0\n",
      "Loss_train: 6.669118993123595e+18\n",
      "Loss_test: 3365753343611596.0\n",
      "Epoch number 1\n",
      "Loss_train: 1291188053516269.0\n",
      "Loss_test: 1558556580684062.5\n",
      "Epoch number 2\n",
      "Loss_train: 497056757585886.56\n",
      "Loss_test: 716181173179771.5\n",
      "Epoch number 3\n",
      "Loss_train: 253119420216680.44\n",
      "Loss_test: 575486993042568.0\n",
      "Epoch number 4\n",
      "Loss_train: 158732122908552.66\n",
      "Loss_test: 424747524508006.06\n",
      "Finished Training\n",
      "Combination number 1\n",
      "Epoch number 0\n",
      "Loss_train: 4772694656908.317\n",
      "Loss_test: 370849591.4446108\n",
      "Epoch number 1\n",
      "Loss_train: 213493608.6145625\n",
      "Loss_test: 93760578.44535927\n",
      "Epoch number 2\n",
      "Loss_train: 74615164.96530664\n",
      "Loss_test: 40981787.99038057\n",
      "Epoch number 3\n",
      "Loss_train: 34072581.944832034\n",
      "Loss_test: 20726352.956972774\n",
      "Epoch number 4\n",
      "Loss_train: 12914517.424693359\n",
      "Loss_test: 13511501.735790139\n",
      "Finished Training\n",
      "Combination number 2\n",
      "Epoch number 0\n",
      "Loss_train: 5960860639982616.0\n",
      "Loss_test: 56467907033.01\n",
      "Epoch number 1\n",
      "Loss_train: 52251756429.22667\n",
      "Loss_test: 18515321632.72\n",
      "Epoch number 2\n",
      "Loss_train: 15902466150.4\n",
      "Loss_test: 7814012543.535\n",
      "Epoch number 3\n",
      "Loss_train: 6766508343.92\n",
      "Loss_test: 6081571389.47\n",
      "Epoch number 4\n",
      "Loss_train: 4051290847.8\n",
      "Loss_test: 3279358851.235\n",
      "Finished Training\n",
      "Search is finished!\n"
     ]
    }
   ],
   "source": [
    "grid_of_params = {'batch_size': [3, 5, 7], 'kernel_sizes': [2, 3, 5], 'n_layers': [3, 4, 5], \n",
    "                  'learning_rate': [0.05, 0.01, 0.1], 'l2_regularization': [0.1, 0.05, 0.01]}\n",
    "\n",
    "good_model, res = RandomizedSearchArchitecture(grid_of_params, data_train, data_test, n_iter = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariousNet(\n",
       "  (hidden): ModuleList(\n",
       "    (0): Conv2d(3, 24, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): Conv2d(24, 36, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (2): Conv2d(36, 48, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (lin1): Linear(in_features=86016, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (lin4): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tImplementing the regularization scheme named \"Cutout\" as proposed in the paper titled, \"Improved Regularization of Convolutional Neural Networks with Cutout\" (landing page here: https:// arxiv.org/abs/1708.04552)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(img, cutout_prob = 0.2):\n",
    "    #img must have shape (n_channels, shape1, shape2)\n",
    "    ToBeOrNotToBe = np.random.binomial(1, cutout_prob)\n",
    "    if ToBeOrNotToBe == 1:\n",
    "        shape1 = img.shape[0]\n",
    "        shape2 = img.shape[1]\n",
    "        x1 = np.random.randint(0, int(shape1*0.8))\n",
    "        y1 = np.random.randint(0, int(shape2*0.8))\n",
    "        x_shape = np.random.randint(1, int(shape1*0.2))\n",
    "        y_shape = np.random.randint(1, int(shape2*0.2))\n",
    "        img[x1:(x1+x_shape), y1:(y1+y_shape), :] = np.zeros((x_shape, y_shape, img.shape[2]))\n",
    "    return img\n",
    "\n",
    "def learning_process_with_cutout(params, data_train, data_val):\n",
    "    #Make dataloaders that will split data to batches for learning\n",
    "    trainLoader = torch.utils.data.DataLoader(data_train, batch_size=params.get('batch_size', 5),\n",
    "                                              shuffle=True, num_workers=1)\n",
    "    valLoader = torch.utils.data.DataLoader(data_val, batch_size=params.get('batch_size', 5),\n",
    "                                             shuffle=False, num_workers=1)\n",
    "    #Check if cuda is available. If it is, then we will do all learning on gpu to make it much faster\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    RegNet = VariousNet(n_layers = params.get('n_layers', 5), kernel_sizes = params.get('kernel_sizes', 3))\n",
    "    RegNet.to(device)\n",
    "    lossMSE = nn.MSELoss(reduce = True, reduction = 'mean')\n",
    "    optimizer = torch.optim.Adam(RegNet.parameters(), lr = params.get('learning_rate', 0.01),\n",
    "                                 weight_decay = params.get('l2_regularization', 0.1))\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch number {}'.format(epoch))\n",
    "        #print('Number of batches is {}'.format(int(len(data_val)/batch_size)))\n",
    "        train_losses = []\n",
    "        RegNet.train() #Change neural network's mode to training mode\n",
    "        for i, d in enumerate(trainLoader):\n",
    "            #load data and make it right shape (batch size, number of channels, shape0, shape1)\n",
    "            in_data = np.array([cutout(io.imread(os.path.join(data_name, name))) for name in d[0]])\n",
    "            in_data = np.swapaxes(in_data, 1, 3)\n",
    "            #if gpu is available, learning will be on gpu\n",
    "            if device.type == 'cuda':\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            else:\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = RegNet.forward(in_data)\n",
    "            loss = lossMSE(outputs, target_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.cpu().item())\n",
    "        train_losses = np.mean(train_losses)\n",
    "        train_err_list.append(train_losses)\n",
    "        print('Loss_train: {}'.format(train_losses))\n",
    "        \n",
    "        test_losses = []\n",
    "        RegNet.eval()\n",
    "        for i, d in enumerate(valLoader):\n",
    "            in_data = np.array([cutout(io.imread(os.path.join(data_name, name))) for name in d[0]])\n",
    "            in_data = np.swapaxes(in_data, 1, 3)\n",
    "            if device.type == 'cuda':\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            else:\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            outputs = RegNet.forward(in_data)\n",
    "            loss = lossMSE(outputs, target_data)\n",
    "            test_losses.append(loss.cpu().item())\n",
    "    \n",
    "        test_losses = np.mean(test_losses)\n",
    "        test_err_list.append(test_losses)\n",
    "        print('Loss_test: {}'.format(test_losses))\n",
    "\n",
    "    print('Finished Training')\n",
    "    return RegNet, test_err_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "Loss_train: 7160.256997307539\n",
      "Loss_test: 1713.0972942848502\n",
      "Epoch number 1\n",
      "Loss_train: 4162.889698725541\n",
      "Loss_test: 3782.00986618042\n",
      "Epoch number 2\n",
      "Loss_train: 4591.108309500218\n",
      "Loss_test: 1230.48572381258\n",
      "Epoch number 3\n",
      "Loss_train: 1.1843448315799107e+17\n",
      "Loss_test: 12458505958778.88\n",
      "Epoch number 4\n",
      "Loss_train: 6246338604400.64\n",
      "Loss_test: 9968887390146.56\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "params = {'n_layers': 5, 'kernel_sizes': 3}\n",
    "cutoutNet, cutoutErr = learning_process_with_cutout(params, data_train, data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tImplementing the regularization scheme titled, \"MixUp\", as proposed in the paper titled, \"mixup: Be- yond Empirical Risk Minimization\" (landing page here: https://arxiv.org/abs/1710. 09412).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(imgs, targets):\n",
    "    coef = np.random.uniform(0, 1)\n",
    "    img_res = imgs[0]*coef + imgs[1]*(1-coef)\n",
    "    target_res = targets[0]*coef + targets[1]*(1-coef)\n",
    "    return img_res, target_res\n",
    "\n",
    "def learning_process_with_mixup(params, data_train, data_val):\n",
    "    #Make dataloaders that will split data to batches for learning\n",
    "    trainLoader = torch.utils.data.DataLoader(data_train, batch_size=params.get('batch_size', 5),\n",
    "                                              shuffle=True, num_workers=1)\n",
    "    valLoader = torch.utils.data.DataLoader(data_val, batch_size=params.get('batch_size', 5),\n",
    "                                             shuffle=False, num_workers=1)\n",
    "    #Check if cuda is available. If it is, then we will do all learning on gpu to make it much faster\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    RegNet = VariousNet(n_layers = params.get('n_layers', 5), kernel_sizes = params.get('kernel_sizes', 3))\n",
    "    RegNet.to(device)\n",
    "    lossMSE = nn.MSELoss(reduce = True, reduction = 'mean')\n",
    "    optimizer = torch.optim.Adam(RegNet.parameters(), lr = params.get('learning_rate', 0.01),\n",
    "                                 weight_decay = params.get('l2_regularization', 0.1))\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch number {}'.format(epoch))\n",
    "        #print('Number of batches is {}'.format(int(len(data_val)/batch_size)))\n",
    "        train_losses = []\n",
    "        RegNet.train() #Change neural network's mode to training mode\n",
    "        for i, d in enumerate(trainLoader):\n",
    "            #load data and make it right shape (batch size, number of channels, shape0, shape1)\n",
    "            in_data = np.array([cutout(io.imread(os.path.join(data_name, name))) for name in d[0]])\n",
    "            in_data = np.swapaxes(in_data, 1, 3)\n",
    "            \n",
    "            target_data = np.array([float(i) for i in d[1]])\n",
    "            \n",
    "            mix1_ind = np.random.randint(0, len(target_data), int(len(target_data)*0.3))\n",
    "            to_mixup1 = in_data[mix1_ind, :, :, :]\n",
    "            to_mixup11 = target_data[mix1_ind]\n",
    "            mix2_ind = np.random.randint(0, len(target_data), int(len(target_data)*0.3))\n",
    "            to_mixup2 = in_data[mix2_ind, :, :, :]\n",
    "            to_mixup21 = target_data[mix2_ind]\n",
    "            \n",
    "            new_data, new_target = mixup((to_mixup1, to_mixup2), (to_mixup11, to_mixup21))\n",
    "            \n",
    "            in_data = np.concatenate((in_data, new_data), axis = 0)\n",
    "            target_data = np.concatenate((target_data, new_target), axis = 0)\n",
    "            #if gpu is available, learning will be on gpu\n",
    "            if device.type == 'cuda':\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            else:\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = RegNet.forward(in_data)\n",
    "            loss = lossMSE(outputs, target_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.cpu().item())\n",
    "        train_losses = np.mean(train_losses)\n",
    "        train_err_list.append(train_losses)\n",
    "        print('Loss_train: {}'.format(train_losses))\n",
    "        \n",
    "        test_losses = []\n",
    "        RegNet.eval()\n",
    "        for i, d in enumerate(valLoader):\n",
    "            in_data = np.array([io.imread(os.path.join(data_name, name)) for name in d[0]])\n",
    "            in_data = np.swapaxes(in_data, 1, 3)\n",
    "            if device.type == 'cuda':\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.cuda.FloatTensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            else:\n",
    "                in_data = torch.cuda.FloatTensor(in_data)\n",
    "                target_data = np.array([float(i) for i in d[1]])\n",
    "                target_data = torch.Tensor(target_data.reshape((target_data.shape[0], 1)))\n",
    "            outputs = RegNet.forward(in_data)\n",
    "            loss = lossMSE(outputs, target_data)\n",
    "            test_losses.append(loss.cpu().item())\n",
    "    \n",
    "        test_losses = np.mean(test_losses)\n",
    "        test_err_list.append(test_losses)\n",
    "        print('Loss_test: {}'.format(test_losses))\n",
    "\n",
    "    print('Finished Training')\n",
    "    return RegNet, test_err_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "Loss_train: 25331464.0863622\n",
      "Loss_test: 5313.5547183990475\n",
      "Epoch number 1\n",
      "Loss_train: 6525.405307235717\n",
      "Loss_test: 1958.419469306469\n",
      "Epoch number 2\n",
      "Loss_train: 4931.019961036046\n",
      "Loss_test: 1846.9464272904397\n",
      "Epoch number 3\n",
      "Loss_train: 5091.423900222779\n",
      "Loss_test: 1472.0443218493463\n",
      "Epoch number 4\n",
      "Loss_train: 3893.8353823216758\n",
      "Loss_test: 2309.1153281229736\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "mixNet, mixErr = learning_process_with_mixup(params, data_train, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
